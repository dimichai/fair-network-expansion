{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Please provide a list of directories `result_dirs` to be evalutated. Use the [table generating section](#results-table) and [plot generating section](#results-plots) to obtain a table and plots respectively. The result metrics for the same hyperparameter settings will be averaged over multiple seed runs (if multiple exist), and the plots will be shown for all seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from main import set_seed\n",
    "from environment import Environment\n",
    "from pathlib import Path\n",
    "from constraints import ForwardConstraints\n",
    "from trainer import Trainer\n",
    "from reward import od_utility\n",
    "\n",
    "from constants import device\n",
    "\n",
    "result_dirs = [\"example-runs/diagonal_5x5_20220820_14_32_40.304876\"]\n",
    "\n",
    "for r in result_dirs:\n",
    "    assert os.path.exists(r), f\"Result dir list contains at least one directory that does not exist: {r}\"\n",
    "\n",
    "def load_state(resdir):\n",
    "    \"\"\"Load the model and corresponding environment to be evaluated. Also returns the arguments.\n",
    "       Args:\n",
    "       ----\n",
    "       resdir (str): the path to the trained model directory.\"\"\"\n",
    "    with open(os.path.join(resdir, \"args.txt\")) as argfile:\n",
    "        args_dict = json.load(argfile)\n",
    "    args = argparse.Namespace(**args_dict)\n",
    "    if args.seed:\n",
    "        set_seed(args.seed)\n",
    "    environment = Environment(Path(f\"./environments/{args.environment}\"), \n",
    "                              groups_file=args.groups_file, \n",
    "                              reward_scaling_fn=args.cf_reward_scaling, \n",
    "                              efficient_station_fn=args.cf_efficient_station,  # Not used\n",
    "                              dmin=args.cf_dmin,  # Not used\n",
    "                              dmax=args.cf_dmax)  # Not used\n",
    "    constraints = ForwardConstraints(environment.grid_x_size, environment.grid_y_size, environment.existing_lines_full, environment.grid_to_vector)\n",
    "    trainer = Trainer(environment, constraints, args)\n",
    "    trainer.actor.eval()\n",
    "\n",
    "    return args, environment, trainer\n",
    "\n",
    "def evaluate_model(args, environment, trainer):\n",
    "    \"\"\"Evaluates a given model on a given environment.\"\"\"\n",
    "\n",
    "    # Setup the initial static and dynamic states.\n",
    "    static = environment.static\n",
    "    dynamic = torch.zeros((1, args.dynamic_size, environment.grid_size),\n",
    "                              device=device).float()  # size with batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Since the line generation process is deterministic due to the greedy sampling\n",
    "        # approach, we only need one line to determine the reward\n",
    "        generated_line, _ = trainer.actor(static, dynamic, args.station_num_lim, decoder_input=None, last_hh=None)\n",
    "        satisfied_od = od_utility(generated_line, environment, args.constraint_free)\n",
    "    \n",
    "    return satisfied_od, generated_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/lo/opt/anaconda3/envs/maskgan/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1634272478997/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/Users/lo/git/fair-network-expansion/environment.py:58: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  grid_x = (vector_idx // self.grid_y_size)\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No groups file provided. Trying to use the default groups file.\n",
      "Number of trainable parameters actor-critic: 182912 / 139181\n",
      "############################## \n",
      "All models evaluated, please note the argument keys below.\n",
      "['hidden_size', 'static_size', 'dynamic_size', 'num_layers', 'dropout', 'checkpoint', 'test', 'epoch_max', 'train_size', 'line_unit_price', 'station_price', 'result_path', 'actor_lr', 'critic_lr', 'actor_mlp_layers', 'critic_mlp_layers', 'station_num_lim', 'budget', 'max_grad_norm', 'environment', 'reward', 'ses_weight', 'var_lambda', 'ggi_weight', 'groups_file', 'arch', 'no_log', 'use_abs', 'early_stopping', 'constraint_free', 'cf_reward_scaling', 'cf_efficient_station', 'cf_station_density', 'cf_dmin', 'cf_dmax', 'plot_every']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = []\n",
    "seeds = defaultdict(list)\n",
    "evaluation = defaultdict(list)\n",
    "\n",
    "# Evaluate all models using their own reward function\n",
    "for resdir in tqdm(result_dirs):\n",
    "    arg, environment, trainer = load_state(resdir)\n",
    "    satisfied_od, generated_line = evaluate_model(arg, environment, trainer)\n",
    "    arg = vars(arg)\n",
    "    seed = arg.pop(\"seed\")\n",
    "\n",
    "    # if the exact arguments (without seed) are already in the list set arg_id to that index\n",
    "    if arg in args:\n",
    "        arg_id = args.index(arg)\n",
    "    # else, make a new arg_id and append the argument settings\n",
    "    else:\n",
    "        arg_id = len(args)\n",
    "        args.append(arg)\n",
    "    seeds[arg_id].append(seed)\n",
    "    evaluation[arg_id].append((satisfied_od, generated_line))\n",
    "\n",
    "print(\"#\"*30, \"\\nAll models evaluated, please note the argument keys below.\")\n",
    "print(list(args[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table\n",
    "Set the column names (`TABLE_COLUMNS`) based on the ouput of the previous cell. The mean od reward and od reward standard deviation are always used. Furthermore, set the desired reward decimal expansion using `rounding`, and the desired column separator using `sep`. The `seed` argument cannot be selected as a column name, since it is used for averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch    actor_mlp_layers    critic_mlp_layers    constraint_free    cf_reward_scaling    mean reward    std. reward\n",
      "poin                   5                    4              False               linear          0.417            0.0\n"
     ]
    }
   ],
   "source": [
    "TABLE_COLUMNS = [\"arch\", \"actor_mlp_layers\", \"critic_mlp_layers\", \"constraint_free\", \"cf_reward_scaling\"]\n",
    "rounding = 3\n",
    "sep = '    '\n",
    "\n",
    "table_header = f\"{sep.join(TABLE_COLUMNS)}{sep}mean reward{sep}std. reward\"\n",
    "table_column_len = [len(c) for c in table_header.split(sep)]\n",
    "print(table_header)\n",
    "\n",
    "for arg_id, arg_setting in enumerate(args):\n",
    "    # Get the rewards\n",
    "    if len(evaluation[arg_id][0]) == 1:\n",
    "        mean_reward = round(evaluation[arg_id][0], rounding)\n",
    "        std_reward = \"n.a.\"\n",
    "    else:\n",
    "        reward_list = np.array([r for r, _ in evaluation[arg_id]])\n",
    "        mean_reward = round(np.mean(reward_list), rounding)\n",
    "        std_reward = round(np.std(reward_list), rounding)\n",
    "    \n",
    "    # Make a list with the table values\n",
    "    entries = [str(arg_setting[val]) for val in TABLE_COLUMNS]\n",
    "    entries.append(str(mean_reward))\n",
    "    entries.append(str(std_reward))\n",
    "\n",
    "    # Truncate the values to prevent cells with too much with, and right pad values that are too short\n",
    "    entries = [s[:l] for s, l in zip(entries, table_column_len)]\n",
    "    entries = [s.rjust(l, ' ') for s, l in zip(entries, table_column_len)]\n",
    "    print(f\"{sep.join(entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results plots\n",
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('maskgan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06f1d07516edbd9007e54bb291d9ebabb54455a8b1b4ee820873b558314f7277"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
